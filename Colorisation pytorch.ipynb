{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modifs by Sacha 18/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sacha : Added 1. Batch norm + 2. Dropout\n",
    "# Sacha : I don't know how to set the dropout â‰  0 in the eval phase \n",
    "# (actually I have an idea, but it's not good to do it)\n",
    "\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # Encoder\n",
    "        # 256 x 256 x 1\n",
    "        self.encod1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=4, padding=1, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "         # 128 x 128 x 64\n",
    "        self.encod2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, padding=1, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )\n",
    "        # 64 x 64 x 128\n",
    "        self.encod3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, padding=1, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "        ) \n",
    "        # 32 x 32 x 256\n",
    "        self.encod4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, padding=1, stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "        )\n",
    "        # 16 x 16 x 512\n",
    "    \n",
    "        # Decoder\n",
    "        self.decod1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.BatchNorm2d(512),\n",
    "        )\n",
    "        # 16 x 16 x 512\n",
    "        self.decod2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=4, padding=1, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.BatchNorm2d(256),\n",
    "        )\n",
    "        # 32 x 32 x 256\n",
    "        self.decod3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, padding=1, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )\n",
    "        # 64 x 64 x 128\n",
    "        self.decod4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, padding=1, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.BatchNorm2d(64),\n",
    "        )\n",
    "        # 128 x 128 x 64\n",
    "        self.decodout = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=64, out_channels=3, kernel_size=4, padding=1, stride=2),\n",
    "            #256 x 256 x 3\n",
    "            nn.Tanh())\n",
    "     \n",
    "    \n",
    "    def forward(self, x):\n",
    "        o1 = self.encod1(x)\n",
    "        o2 = self.encod2(o1)\n",
    "        o3 = self.encod3(o2)\n",
    "        o4 = self.encod4(o3)\n",
    "    \n",
    "        o5 = self.decod1(o4) + o4\n",
    "        o6 = self.decod2(o5) + o3\n",
    "        o7 = self.decod3(o6) + o2 \n",
    "        o8 = self.decod4(o7) + o1\n",
    "        out = self.decodout(o8)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('/Users/sachaizadi/Desktop/man.jpeg')\n",
    "image = np.array(image)[:,:,2]\n",
    "image = torch.from_numpy(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image.type(torch.FloatTensor).resize_(2, 1, 416, 416)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 416, 416])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Generator()\n",
    "model.train()\n",
    "test = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0302, -0.4196,  0.1739,  ..., -0.6910, -0.7610, -0.4488],\n",
       "          [-0.0532, -0.2185,  0.7842,  ..., -0.7984, -0.3094, -0.3748],\n",
       "          [-0.4358,  0.1778,  0.5143,  ...,  0.4868, -0.8784,  0.1371],\n",
       "          ...,\n",
       "          [-0.0319, -0.0505, -0.0532,  ..., -0.0065, -0.0686, -0.0468],\n",
       "          [-0.0603, -0.0349, -0.0750,  ...,  0.0119, -0.0508, -0.0459],\n",
       "          [ 0.0072, -0.0034, -0.0663,  ..., -0.0366, -0.0736,  0.0094]],\n",
       "\n",
       "         [[ 0.5509, -0.3654,  0.0410,  ...,  0.4023, -0.0541,  0.1148],\n",
       "          [-0.4612, -0.1464, -0.5600,  ..., -0.2965, -0.5570, -0.3555],\n",
       "          [ 0.5735,  0.1425, -0.0369,  ...,  0.5969,  0.3961,  0.4859],\n",
       "          ...,\n",
       "          [ 0.0059,  0.0953, -0.0342,  ...,  0.0645, -0.0175,  0.0845],\n",
       "          [ 0.0363,  0.0237,  0.0661,  ...,  0.0900,  0.0679, -0.0935],\n",
       "          [ 0.0073,  0.0680,  0.0328,  ...,  0.0723,  0.0363,  0.0606]],\n",
       "\n",
       "         [[-0.0615,  0.4077, -0.0556,  ...,  0.7786, -0.6226, -0.6409],\n",
       "          [-0.2980,  0.7472, -0.6823,  ..., -0.4251,  0.7217, -0.4949],\n",
       "          [ 0.3139,  0.7736,  0.3312,  ..., -0.1327,  0.6524,  0.8653],\n",
       "          ...,\n",
       "          [ 0.0609,  0.0111, -0.0176,  ..., -0.0032, -0.0111,  0.0774],\n",
       "          [ 0.0252,  0.0259, -0.0049,  ...,  0.0669,  0.0473,  0.0103],\n",
       "          [ 0.0119,  0.0259, -0.0315,  ...,  0.0284, -0.0402,  0.0520]]],\n",
       "\n",
       "\n",
       "        [[[-0.0175,  0.0140, -0.0587,  ...,  0.0767, -0.0135, -0.0252],\n",
       "          [-0.0421,  0.0057, -0.0080,  ..., -0.2188,  0.1049, -0.0697],\n",
       "          [-0.0649,  0.0486, -0.0718,  ...,  0.1109, -0.0345,  0.0213],\n",
       "          ...,\n",
       "          [-0.0486, -0.1771,  0.0089,  ..., -0.0496, -0.0362, -0.0701],\n",
       "          [ 0.0453, -0.0382,  0.0198,  ...,  0.0526, -0.1231,  0.0402],\n",
       "          [ 0.0006,  0.0071, -0.0965,  ..., -0.0639, -0.0413, -0.0124]],\n",
       "\n",
       "         [[ 0.0081, -0.0211,  0.0225,  ..., -0.0292, -0.0158, -0.0155],\n",
       "          [ 0.0053,  0.0816,  0.0334,  ...,  0.1909,  0.0309,  0.0880],\n",
       "          [ 0.0336,  0.0757,  0.0484,  ...,  0.0516,  0.0317, -0.0674],\n",
       "          ...,\n",
       "          [-0.0016,  0.0899, -0.1020,  ...,  0.2170, -0.0410,  0.1237],\n",
       "          [-0.0074,  0.0353,  0.0475,  ...,  0.0664,  0.0421, -0.1021],\n",
       "          [ 0.0103,  0.0603,  0.0020,  ...,  0.0586,  0.0512,  0.0623]],\n",
       "\n",
       "         [[ 0.0729,  0.0199, -0.0424,  ...,  0.0025, -0.0223, -0.0085],\n",
       "          [ 0.0308, -0.0412,  0.0893,  ...,  0.0960,  0.0232,  0.0748],\n",
       "          [-0.0191,  0.0406,  0.0720,  ...,  0.0272, -0.0324,  0.0095],\n",
       "          ...,\n",
       "          [ 0.0687, -0.0307, -0.0155,  ...,  0.0421, -0.0402,  0.0620],\n",
       "          [ 0.0884, -0.0469,  0.0233,  ...,  0.0918,  0.0313,  0.0410],\n",
       "          [ 0.0200,  0.0422,  0.0124,  ...,  0.0464, -0.0521,  0.0481]]]],\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 3.0902e-01, -5.1120e-01,  1.8097e-01,  ..., -7.6587e-01,\n",
       "           -9.0539e-01, -3.1177e-01],\n",
       "          [ 3.1504e-01, -7.2810e-01,  9.9943e-01,  ..., -9.1368e-01,\n",
       "            6.4699e-01, -8.8769e-01],\n",
       "          [-3.8577e-01,  9.4852e-03,  9.1213e-01,  ..., -2.5096e-01,\n",
       "           -7.8961e-01,  7.9981e-01],\n",
       "          ...,\n",
       "          [-2.8085e-02, -3.2910e-02, -2.9058e-02,  ..., -3.2692e-02,\n",
       "           -2.8857e-02, -3.8452e-02],\n",
       "          [-3.8178e-02,  7.3773e-03, -1.9104e-02,  ...,  7.2986e-03,\n",
       "           -1.9531e-02, -1.5214e-02],\n",
       "          [-1.9346e-02, -2.0680e-02, -1.9141e-02,  ..., -2.0522e-02,\n",
       "           -1.9059e-02, -2.6532e-02]],\n",
       "\n",
       "         [[ 8.0624e-01, -1.3629e-01, -2.5619e-01,  ...,  1.3658e-01,\n",
       "            5.5926e-02, -2.4281e-01],\n",
       "          [-9.0311e-01, -3.6824e-01, -8.9633e-01,  ..., -6.6439e-01,\n",
       "           -5.0524e-01, -8.9193e-01],\n",
       "          [ 8.8117e-01,  4.1550e-01, -5.5284e-01,  ...,  7.5034e-01,\n",
       "           -2.7141e-02,  7.1162e-01],\n",
       "          ...,\n",
       "          [-3.7799e-03,  3.5958e-02,  1.1465e-02,  ...,  3.5822e-02,\n",
       "            1.1609e-02,  5.2328e-02],\n",
       "          [ 1.6071e-02, -1.3986e-03,  2.0776e-02,  ..., -1.9211e-03,\n",
       "            2.1164e-02, -1.2409e-02],\n",
       "          [ 5.7352e-03,  3.3461e-02,  1.5896e-02,  ...,  3.3702e-02,\n",
       "            1.6334e-02,  3.6556e-02]],\n",
       "\n",
       "         [[-3.1051e-02,  8.4515e-01, -1.7645e-01,  ...,  9.7282e-01,\n",
       "           -7.6655e-01, -4.5102e-01],\n",
       "          [-8.2285e-01,  9.8284e-01, -9.5917e-01,  ...,  4.9807e-01,\n",
       "            9.1889e-01, -6.0293e-01],\n",
       "          [ 4.5580e-01,  9.9231e-01,  9.1291e-01,  ...,  4.6563e-01,\n",
       "            8.8821e-01,  9.8375e-01],\n",
       "          ...,\n",
       "          [ 3.0676e-02,  1.9418e-02,  1.3321e-02,  ...,  1.9498e-02,\n",
       "            1.3146e-02,  2.8101e-02],\n",
       "          [ 2.6160e-02,  2.5442e-02,  3.5056e-02,  ...,  2.6046e-02,\n",
       "            3.5094e-02,  2.8701e-02],\n",
       "          [ 2.7636e-02,  2.1750e-02,  1.5404e-02,  ...,  2.1904e-02,\n",
       "            1.5260e-02,  2.5804e-02]]]], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = image.type(torch.FloatTensor).resize_(1, 1, 416, 416)\n",
    "model.eval()\n",
    "model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.conv= nn.Sequential(\n",
    "           # 256 x 256 x 3\n",
    "\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            \n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),  \n",
    "            nn.BatchNorm2d(128),\n",
    "            \n",
    "            \n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            \n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.ELU(),\n",
    "            # 256 x 256 x 512\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features = 256*256*512, out_features = 1 ),\n",
    "        )\n",
    "      \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, 256*256*512)\n",
    "        x = self.fc(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('/Users/sachaizadi/Desktop/man.jpeg')\n",
    "image = np.array(image)[:,:,]\n",
    "image = torch.from_numpy(image)\n",
    "image = image.type(torch.FloatTensor).resize_(2, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4993],\n",
      "        [0.4993]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.4995],\n",
      "        [0.4995]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = Discriminator()\n",
    "model.train()\n",
    "test = model(image)\n",
    "print(test)\n",
    "model.eval()\n",
    "test = model(image)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(object):\n",
    "    def __init__(self, config):      \n",
    "        self.config = config\n",
    "        self.data_init()\n",
    "        self.model_init()\n",
    "        \n",
    "    def data_init(self):\n",
    "        self.dataset = Dataset_Color()\n",
    "        self.training_data_loader = DataLoader(dataset= self.dataset, batch_size= self.config.batch_size, shuffle=True)\n",
    "        \n",
    "    def model_init(self):\n",
    "        self.dis_model = Discriminator()\n",
    "        self.dis_model = self.dis_model.cuda()\n",
    "        self.gen_model = Generator()\n",
    "        self.gen_model = self.gen_model.cuda()\n",
    "        self.loss = nn.BCELoss(size_average=False).cuda()\n",
    "        self.optimizer_dis = optim.Adam(self.dis_model.parameters(),lr= self.config.lr_dis,betas=self.config.betas)\n",
    "        self.optimizer_gen = optim.Adam(self.gen_model.parameters(),lr= self.config.lr_gen,betas=self.config.betas)\n",
    "        \n",
    "    def train(self):\n",
    "        return self.training( self.dis_model, self.gen_model, self.training_data_loader, self.loss, self.optimizer_dis, self.optimizer_gen,n_epochs = self.config.n_epochs)\n",
    "      \n",
    "        \n",
    "    def training(self,dis_model, gen_model, data_loader, loss_fn, \n",
    "              dis_optimizer, gen_optimizer, n_epochs= 1):\n",
    "        \n",
    "        use_gpu =  torch.cuda.is_available()\n",
    "        if use_gpu == True:\n",
    "            torch.cuda.set_device(0)\n",
    "\n",
    "\n",
    "        dis_model.train(True)\n",
    "        gen_model.train(True)\n",
    "\n",
    "        dis_loss = np.zeros(n_epochs)\n",
    "        gen_loss = np.zeros(n_epochs)\n",
    "\n",
    "        for epoch_num in range(n_epochs):\n",
    "\n",
    "            dis_running_loss = 0.0\n",
    "            gen_running_loss = 0.0\n",
    "            size = 0\n",
    "\n",
    "            for data in data_loader:\n",
    "\n",
    "                clr_img, bw_img = data['clr'], data['bw']\n",
    "\n",
    "                if use_gpu == True:\n",
    "                    clr_img = clr_img.cuda()\n",
    "                    bw_img  = bw_img.cuda()\n",
    "\n",
    "                batch_size = clr_img.size(0)\n",
    "                size += batch_size\n",
    "                # get image size            \n",
    "                # reset optimizers\n",
    "                dis_optimizer.zero_grad()\n",
    "                gen_optimizer.zero_grad()\n",
    "\n",
    "                # TRAIN THE DISCRIMINATOR\n",
    "                out = dis_model(clr_img)\n",
    "\n",
    "                loss = loss_fn(out, torch.ones(batch_size, 1).cuda()) # check size of torch.ones\n",
    "                loss.backward()\n",
    "                dis_running_loss += loss.data.cpu().numpy()\n",
    "\n",
    "                fake_img = gen_model(bw_img).detach()\n",
    "                fake_out = dis_model(fake_img)\n",
    "                loss = loss_fn(fake_out, torch.zeros(batch_size, 1).cuda()) # check size of torch.zeros\n",
    "                dis_running_loss += loss.data.cpu().numpy()\n",
    "                loss.backward()\n",
    "                dis_optimizer.step()\n",
    "\n",
    "\n",
    "                fake_img = gen_model(bw_img).detach()\n",
    "                fake_out = dis_model(fake_img)\n",
    "                loss = loss_fn(fake_out, torch.ones(batch_size, 1).cuda())\n",
    "                gen_running_loss += loss.data.cpu().numpy()\n",
    "                loss.backward()\n",
    "                gen_optimizer.step() #Added by Sacha\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            epoch_dis_loss = dis_running_loss / size\n",
    "            epoch_gen_loss = gen_running_loss / size\n",
    "            dis_loss[epoch_num] = epoch_dis_loss\n",
    "            gen_loss[epoch_num] = epoch_gen_loss\n",
    "\n",
    "            if (epoch_num+1) % 50 == 0 and epoch_num!=0: \n",
    "                print('Train - Discriminator Loss: {:.4f} Generator Loss: {:.4f}'.format(epoch_dis_loss, epoch_gen_loss))\n",
    "                \n",
    "            if (epoch_num+1) % self.config.save_frequency == 0 and epoch_num !=0: \n",
    "                if not os.path.exists(self.config.model_dir):\n",
    "                    os.makedirs(self.config.model_dir)\n",
    "                state = {'epoch': epoch_num, 'gen_model_state_dict': gen_model.state_dict(), 'dis_model_state_dict': dis_model.state_dict(), 'dis_optimizer': dis_optimizer, 'gen_optimizer': gen_optimizer}\n",
    "                torch.save(state, self.config.model_dir + 'model_weights_epoch_%i.pk' %epoch_num)\n",
    "                print (\"Saved Model\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
